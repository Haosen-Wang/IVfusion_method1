Epoch 1/5:   2%|█▍                                                                      | 3/150 [00:07<06:16,  2.56s/it]
当前学习率: 0.001000
Traceback (most recent call last):
  File "/home/user/1024_whs/IVfusion_method1/model/stage1/train.py", line 414, in <module>
    main(**config_dic)
  File "/home/user/1024_whs/IVfusion_method1/model/stage1/train.py", line 379, in main
    best_epoch, best_loss = train_model(
                            ^^^^^^^^^^^^
  File "/home/user/1024_whs/IVfusion_method1/model/stage1/train.py", line 134, in train_model
    epoch_loss = train_epoch_model(model, train_loader, criterion, optimizer, device_1, device_2, val_loader, pbar)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/1024_whs/IVfusion_method1/model/stage1/train.py", line 66, in train_epoch_model
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py", line 38, in _no_grad_wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py", line 218, in clip_grad_norm_
    grads = [p.grad for p in parameters if p.grad is not None]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py", line 218, in <listcomp>
    grads = [p.grad for p in parameters if p.grad is not None]
                                           ^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
