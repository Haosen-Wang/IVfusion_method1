Epoch 1/3:  19%|█████████████▋                                                         | 29/150 [00:02<00:05, 22.11it/s]
当前学习率: 0.000655
⚠️  批次 0 检测到 NaN/Inf 损失，跳过此批次...
损失详情: {'total_loss_f': tensor(nan, device='cuda:1', grad_fn=<AddBackward0>), 'pixel_loss': tensor(nan, device='cuda:1', grad_fn=<MeanBackward0>), 'gradient_loss': tensor(nan, device='cuda:1', grad_fn=<MeanBackward0>), 'perceptual_loss': tensor(nan, device='cuda:1', grad_fn=<MeanBackward0>), 'total_loss_vi': tensor(nan, device='cuda:1', grad_fn=<MulBackward0>), 'rec_n': tensor(5.5452, device='cuda:1'), 'kl_loss_n': tensor(nan, device='cuda:1', grad_fn=<MulBackward0>), 'total_loss': tensor(nan, device='cuda:1', grad_fn=<AddBackward0>)}
⚠️  批次 1 内存不足，跳过...
⚠️  批次 2 内存不足，跳过...
⚠️  批次 3 内存不足，跳过...
⚠️  批次 4 内存不足，跳过...
⚠️  批次 5 内存不足，跳过...
⚠️  批次 6 内存不足，跳过...
⚠️  批次 7 内存不足，跳过...
⚠️  批次 8 内存不足，跳过...
⚠️  批次 9 内存不足，跳过...
⚠️  批次 10 内存不足，跳过...
⚠️  批次 11 内存不足，跳过...
⚠️  批次 12 内存不足，跳过...
⚠️  批次 13 内存不足，跳过...
⚠️  批次 14 内存不足，跳过...
⚠️  批次 15 内存不足，跳过...
⚠️  批次 16 内存不足，跳过...
⚠️  批次 17 内存不足，跳过...
⚠️  批次 18 内存不足，跳过...
⚠️  批次 19 内存不足，跳过...
⚠️  批次 20 内存不足，跳过...
⚠️  批次 21 内存不足，跳过...
⚠️  批次 22 内存不足，跳过...
⚠️  批次 23 内存不足，跳过...
⚠️  批次 24 内存不足，跳过...
⚠️  批次 25 内存不足，跳过...
⚠️  批次 26 内存不足，跳过...
⚠️  批次 27 内存不足，跳过...
⚠️  批次 28 内存不足，跳过...
⚠️  批次 29 内存不足，跳过...
⚠️  批次 30 内存不足，跳过...
⚠️  批次 31 内存不足，跳过...
⚠️  批次 32 内存不足，跳过...
⚠️  批次 33 内存不足，跳过...
⚠️  批次 34 内存不足，跳过...
⚠️  批次 35 内存不足，跳过...
⚠️  批次 36 内存不足，跳过...
⚠️  批次 37 内存不足，跳过...
⚠️  批次 38 内存不足，跳过...
⚠️  批次 39 内存不足，跳过...
⚠️  批次 40 内存不足，跳过...
⚠️  批次 41 内存不足，跳过...
⚠️  批次 42 内存不足，跳过...
⚠️  批次 43 内存不足，跳过...
⚠️  批次 44 内存不足，跳过...
⚠️  批次 45 内存不足，跳过...
⚠️  批次 46 内存不足，跳过...
⚠️  批次 47 内存不足，跳过...
⚠️  批次 48 内存不足，跳过...
⚠️  批次 49 内存不足，跳过...
⚠️  批次 50 内存不足，跳过...
⚠️  批次 51 内存不足，跳过...
⚠️  批次 52 内存不足，跳过...
⚠️  批次 53 内存不足，跳过...
⚠️  批次 54 内存不足，跳过...
⚠️  批次 55 内存不足，跳过...
⚠️  批次 56 内存不足，跳过...
⚠️  批次 57 内存不足，跳过...
⚠️  批次 58 内存不足，跳过...
⚠️  批次 59 内存不足，跳过...
⚠️  批次 60 内存不足，跳过...
⚠️  批次 61 内存不足，跳过...
⚠️  批次 62 内存不足，跳过...
⚠️  批次 63 内存不足，跳过...
⚠️  批次 64 内存不足，跳过...
⚠️  批次 65 内存不足，跳过...
⚠️  批次 66 内存不足，跳过...
⚠️  批次 67 内存不足，跳过...
⚠️  批次 68 内存不足，跳过...
⚠️  批次 69 内存不足，跳过...
⚠️  批次 70 内存不足，跳过...
⚠️  批次 71 内存不足，跳过...
⚠️  批次 72 内存不足，跳过...
⚠️  批次 73 内存不足，跳过...
⚠️  批次 74 内存不足，跳过...
⚠️  批次 75 内存不足，跳过...
⚠️  批次 76 内存不足，跳过...
⚠️  批次 77 内存不足，跳过...
⚠️  批次 78 内存不足，跳过...
⚠️  批次 79 内存不足，跳过...
⚠️  批次 80 内存不足，跳过...
⚠️  批次 81 内存不足，跳过...
⚠️  批次 82 内存不足，跳过...
⚠️  批次 83 内存不足，跳过...
⚠️  批次 84 内存不足，跳过...
⚠️  批次 85 内存不足，跳过...
⚠️  批次 86 内存不足，跳过...
⚠️  批次 87 内存不足，跳过...
⚠️  批次 88 内存不足，跳过...
⚠️  批次 89 内存不足，跳过...
⚠️  批次 90 内存不足，跳过...
⚠️  批次 91 内存不足，跳过...
⚠️  批次 92 内存不足，跳过...
⚠️  批次 93 内存不足，跳过...
⚠️  批次 94 内存不足，跳过...
⚠️  批次 95 内存不足，跳过...
⚠️  批次 96 内存不足，跳过...
⚠️  批次 97 内存不足，跳过...
⚠️  批次 98 内存不足，跳过...
⚠️  批次 99 内存不足，跳过...
⚠️  批次 100 内存不足，跳过...
⚠️  批次 101 内存不足，跳过...
⚠️  批次 102 内存不足，跳过...
⚠️  批次 103 内存不足，跳过...
⚠️  批次 104 内存不足，跳过...
⚠️  批次 105 内存不足，跳过...
⚠️  批次 106 内存不足，跳过...
⚠️  批次 107 内存不足，跳过...
⚠️  批次 108 内存不足，跳过...
⚠️  批次 109 内存不足，跳过...
⚠️  批次 110 内存不足，跳过...
⚠️  批次 111 内存不足，跳过...
⚠️  批次 112 内存不足，跳过...
⚠️  批次 113 内存不足，跳过...
⚠️  批次 114 内存不足，跳过...
⚠️  批次 115 内存不足，跳过...
⚠️  批次 116 内存不足，跳过...
⚠️  批次 117 内存不足，跳过...
⚠️  批次 118 内存不足，跳过...
⚠️  批次 119 内存不足，跳过...
⚠️  批次 120 内存不足，跳过...
⚠️  批次 121 内存不足，跳过...
⚠️  批次 122 内存不足，跳过...
⚠️  批次 123 内存不足，跳过...
⚠️  批次 124 内存不足，跳过...
⚠️  批次 125 内存不足，跳过...
⚠️  批次 126 内存不足，跳过...
⚠️  批次 127 内存不足，跳过...
⚠️  批次 128 内存不足，跳过...
⚠️  批次 129 内存不足，跳过...
⚠️  批次 130 内存不足，跳过...
⚠️  批次 131 内存不足，跳过...
⚠️  批次 132 内存不足，跳过...
⚠️  批次 133 内存不足，跳过...
⚠️  批次 134 内存不足，跳过...
⚠️  批次 135 内存不足，跳过...
⚠️  批次 136 内存不足，跳过...
⚠️  批次 137 内存不足，跳过...
⚠️  批次 138 内存不足，跳过...
⚠️  批次 139 内存不足，跳过...
⚠️  批次 140 内存不足，跳过...
⚠️  批次 141 内存不足，跳过...
⚠️  批次 142 内存不足，跳过...
⚠️  批次 143 内存不足，跳过...
⚠️  批次 144 内存不足，跳过...
⚠️  批次 145 内存不足，跳过...
⚠️  批次 146 内存不足，跳过...
⚠️  批次 147 内存不足，跳过...
⚠️  批次 148 内存不足，跳过...
⚠️  批次 149 内存不足，跳过...
/opt/conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(
✓ 新的最佳模型已保存！ (Epoch 1)
Epoch [1/3],Train Loss: 0.0000 🏆
Epoch 2/3:   6%|████▎                                                                   | 9/150 [00:01<00:26,  5.39it/s]
当前学习率: 0.000491
⚠️  批次 0 检测到 NaN/Inf 损失，跳过此批次...
损失详情: {'total_loss_f': tensor(nan, device='cuda:1', grad_fn=<AddBackward0>), 'pixel_loss': tensor(nan, device='cuda:1', grad_fn=<MeanBackward0>), 'gradient_loss': tensor(nan, device='cuda:1', grad_fn=<MeanBackward0>), 'perceptual_loss': tensor(nan, device='cuda:1', grad_fn=<MeanBackward0>), 'total_loss_vi': tensor(nan, device='cuda:1', grad_fn=<MulBackward0>), 'rec_n': tensor(5.5452, device='cuda:1'), 'kl_loss_n': tensor(nan, device='cuda:1', grad_fn=<MulBackward0>), 'total_loss': tensor(nan, device='cuda:1', grad_fn=<AddBackward0>)}
⚠️  批次 1 内存不足，跳过...
⚠️  批次 2 内存不足，跳过...
⚠️  批次 3 内存不足，跳过...
⚠️  批次 4 内存不足，跳过...
⚠️  批次 5 内存不足，跳过...
⚠️  批次 6 内存不足，跳过...
⚠️  批次 7 内存不足，跳过...
⚠️  批次 8 内存不足，跳过...
Traceback (most recent call last):
  File "/home/user/1024_whs/IVfusion_method1/model/stage1/train_dc.py", line 122, in train_epoch_model
    Ic_image, n, mu_n, sigma2_n =model(d_image,device_1, device_2)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/1024_whs/IVfusion_method1/model/stage1/model.py", line 37, in forward
    n,mu_n,sigma2_n = self.noise_encoder_decoder(image)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/1024_whs/IVfusion_method1/model/stage1/model.py", line 22, in forward
    x=self.encoder(x)
      ^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/1024_whs/IVfusion_method1/model/stage1/component.py", line 19, in forward
    x=self.model(x)
      ^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/container.py", line 240, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/1024_whs/IVfusion_method1/model/stage1/../restormer/restormer_arch.py", line 265, in forward
    out_dec_level1 = self.decoder_level1(inp_dec_level1)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/container.py", line 240, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/1024_whs/IVfusion_method1/model/stage1/../restormer/restormer_arch.py", line 142, in forward
    x = x + self.ffn(self.norm2(x))
            ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/1024_whs/IVfusion_method1/model/stage1/../restormer/restormer_arch.py", line 85, in forward
    x = F.gelu(x1) * x2
        ~~~~~~~~~~~^~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 23.53 GiB of which 13.06 MiB is free. Including non-PyTorch memory, this process has 23.49 GiB memory in use. Of the allocated memory 22.69 GiB is allocated by PyTorch, and 361.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/user/1024_whs/IVfusion_method1/model/stage1/train_dc.py", line 570, in <module>
    main(**config_dic)
  File "/home/user/1024_whs/IVfusion_method1/model/stage1/train_dc.py", line 462, in main
    best_epoch, best_loss = train_model(
                            ^^^^^^^^^^^^
  File "/home/user/1024_whs/IVfusion_method1/model/stage1/train_dc.py", line 217, in train_model
    epoch_loss = train_epoch_model(model, train_loader, criterion, optimizer, device_1, device_2, val_loader, pbar)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/user/1024_whs/IVfusion_method1/model/stage1/train_dc.py", line 163, in train_epoch_model
    if "out of memory" in str(e):
                          ^^^^^^
KeyboardInterrupt
